import requests, cloudscraper, time, json, os, random, backoff, yt_dlp, sys, subprocess
from fake_useragent import UserAgent
from tqdm import tqdm
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# ==========================================================
# ğŸ‘‘ THE ARCHIVAL FUSION REACTOR - VERSION 25.0 [ULTIMATE]
# ==========================================================
# OWNER: SOVEREIGN ARCHIVIST
# TARGET: @drFathysaid (TOTAL DOMINANCE)
# FEATURES: LIVE, SHORTS, PODCASTS, PIXEL-PERFECTION
# ==========================================================

class SovereignFusionReactor:
    def __init__(self):
        self.ua = UserAgent()
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
        )
        self.auth = "LOW 0QXWMqn3plltxkHY:EBZOMdJmS6WKfCzp"
        self.input_file = "dr_fathy_ALL_links_oldest_to_newest.txt"
        self.vault_file = "TOTAL_IMMORTALITY_VAULT_V25.json"
        self.error_log = "reactor_errors.log"
        self.cookies_file = "cookies.txt"
        self.setup_environment()

    def setup_environment(self):
        """ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª"""
        print(f"ğŸš€ [SYSTEM] BOOTING REACTOR V25.0 AT {datetime.now()}")
        if not os.path.exists(self.vault_file):
            with open(self.vault_file, 'w', encoding='utf-8') as f:
                json.dump({"metadata": {"project": "Dr. Fathy Said", "version": 25.0}, "entries": {}}, f)
        
        if not os.path.exists(self.input_file):
            print(f"âš ï¸ [WARN] Input file {self.input_file} missing. System will wait.")

    def _get_stealth_headers(self, direct_url=None):
        """ØªÙˆÙ„ÙŠØ¯ Ù‡ÙŠØ¯Ø±Ø² ØªØ®ÙÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù„ÙƒØ³Ø± Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØªØ¨Ø¹"""
        headers = {
            "Authorization": self.auth,
            "User-Agent": self.ua.random,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "X-Archive-Wayback-Runtime-Compatibility": "1",
            "X-Archive-Source-Stream": direct_url if direct_url else "",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }
        return headers

    @backoff.on_exception(backoff.expo, Exception, max_tries=15)
    def extract_pure_dna(self, url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø±Ø³ÙˆØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŒ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŒ Ø§Ù„Ø¬ÙˆØ¯Ø©)"""
        ydl_opts = {
            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
            'quiet': True,
            'no_warnings': True,
            'getcomments': True,
            'cookiefile': self.cookies_file if os.path.exists(self.cookies_file) else None
        }
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            return {
                "direct_url": info.get('url'),
                "title": info.get('title'),
                "duration": info.get('duration'),
                "comments": len(info.get('comments', [])),
                "is_live": info.get('is_live', False),
                "full_data": info
            }

    @backoff.on_exception(backoff.expo, Exception, max_tries=50)
    def execute_force_archive(self, url, dna):
        """Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ù‚Ù† Ø§Ù„Ù‚Ø³Ø±ÙŠ ÙÙŠ Wayback Machine Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø®Ù„ÙˆØ¯"""
        save_api = "https://web.archive.org/save/"
        payload = {
            "url": url,
            "capture_all": "on",            # Ø§Ù„Ø£Ø²Ø±Ø§Ø±ØŒ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŒ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©
            "capture_outlinks": "on",       # Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙŠØ¯ÙŠØ§ Ø§Ù„Ø®Ø§Ù…
            "js_snapshot": "on",            # Ø±Ù†Ø¯Ø± ÙƒØ§Ù…Ù„ Ù„Ù„Ø¬Ø§ÙØ§ Ø³ÙƒØ±ÙŠØ¨Øª
            "save_metadata": "on",          # Ø§Ù„Ù…ÙŠØªØ§-Ø¯Ø§ØªØ§ Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠØ©
            "force_get": "on",              # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù‚Ø¯ÙŠÙ…
            "capture_screenshot": "on",     # Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© 4K
            "outlinks_availability": "on"   # Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ø¹Ù„Ù‰ Ø§Ø³ØªØ¶Ø§ÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª
        }
        
        headers = self._get_stealth_headers(dna['direct_url'])
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù‡Ø¬ÙˆÙ…
        response = self.scraper.post(save_api, data=payload, headers=headers, timeout=600)
        
        if response.status_code == 200:
            return True
        elif response.status_code == 429:
            print("\nğŸ›‘ [SHIELD ACTIVATED] Rate limit hit. Cooling down...")
            time.sleep(900) # Ø§Ù†ØªØ¸Ø§Ø± 15 Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„ØªØ¨Ø±ÙŠØ¯
            raise Exception("Rate Limit")
        return False

    def run_reactor(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙØ§Ø¹Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        with open(self.input_file, 'r', encoding='utf-8') as f:
            links = list(dict.fromkeys([line.strip() for line in f if line.strip()]))

        print(f"ğŸ”± TOTAL ATOMS (LINKS) TO PROCESS: {len(links)}")
        
        with tqdm(total=len(links), desc="âš›ï¸ FUSION IN PROGRESS", colour="yellow", bar_format="{l_bar}{bar:30}{r_bar}") as pbar:
            for url in links:
                v_id = url.split("v=")[-1] if "v=" in url else url
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø®Ø²Ù†Ø©
                with open(self.vault_file, 'r', encoding='utf-8') as f:
                    vault = json.load(f)
                
                if v_id in vault["entries"] and vault["entries"][v_id]["status"] == "IMMORTAL":
                    pbar.update(1)
                    continue

                try:
                    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù€ DNA
                    dna = self.extract_pure_dna(url)
                    
                    # 2. Ø­Ù‚Ù† Ø§Ù„Ø£Ø±Ø´ÙØ©
                    success = self.execute_force_archive(url, dna)
                    
                    # 3. ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Ø¬Ø§Ø­
                    vault["entries"][v_id] = {
                        "status": "IMMORTAL" if success else "FAILED",
                        "title": dna["title"],
                        "duration": dna["duration"],
                        "comments_found": dna["comments"],
                        "archive_link": f"https://web.archive.org/web/*/{url}",
                        "timestamp": str(datetime.now())
                    }
                    with open(self.vault_file, 'w', encoding='utf-8') as f:
                        json.dump(vault, f, indent=4, ensure_ascii=False)
                    
                    pbar.set_postfix({"Secured": dna["title"][:20]})
                except Exception as e:
                    with open(self.error_log, 'a') as f:
                        f.write(f"{datetime.now()} - Error for {url}: {str(e)}\n")
                
                pbar.update(1)
                # Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ (Ù‡Ø§Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ø«Ø¨Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©)
                time.sleep(random.randint(90, 110))

if __name__ == "__main__":
    reactor = SovereignFusionReactor()
    reactor.run_reactor()
